import math
import random
import json
import time
from PushBattle import Game, PLAYER1, PLAYER2, EMPTY, BOARD_SIZE, NUM_PIECES
import os

class MCTSNode:
    def __init__(self, game, parent=None, move=None):
        self.game = game
        self.parent = parent
        self.move = move
        self.children = []
        self.visits = 0
        self.score = 0

class MCTS2Agent:
    def __init__(self, player=PLAYER1, iterations=1000, exploration_weight=1.4, learning_rate=0.1, discount_factor=0.9, q_value_file="q_values.json"):
        self.player = player
        self.iterations = iterations
        self.exploration_weight = exploration_weight
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.q_values = {}  # Q-values for training (state-action pairs)
        self.q_value_file = q_value_file
        self.load_q_values()  # Load Q-values if available

    def save_q_values(self):
        with open(self.q_value_file, 'w') as f:
            json.dump(self.q_values, f)
        print(f"Q-values saved to {self.q_value_file}")

    def load_q_values(self):
        if os.path.exists(self.q_value_file):
            with open(self.q_value_file, 'r') as f:
                self.q_values = json.load(f)
            print(f"Q-values loaded from {self.q_value_file}")
        else:
            print("No existing Q-values found; starting fresh.")

    def get_best_move(self, game):
        start_time = time.time()
        root = MCTSNode(game)
        for i in range(self.iterations):
            node = self.select(root)
            score = self.simulate(node.game)
            self.backpropagate(node, score)
            if time.time() - start_time > 3.5:  # Stop if close to 4-second limit
                print(f"Stopped after {i} iterations due to time constraint")
                break

        if not root.children:
            print("No valid moves found. Returning random move.")
            return self.get_random_valid_move(game)

        best_child = max(root.children, key=lambda c: c.visits)
        best_move = best_child.move

        print(f"MCTS completed {i+1} iterations in {time.time() - start_time:.2f} seconds")
        print(f"Best move: {best_move} with {best_child.visits} visits and score {best_child.score}")

        # Ensure the move is in the correct format
        if isinstance(best_move, tuple):
            best_move = list(best_move)
        
        if len(best_move) == 2 or len(best_move) == 4:
            return [int(x) for x in best_move]  # Ensure all elements are integers
        else:
            print(f"Invalid move generated by MCTS: {best_move}")
            return self.get_random_valid_move(game)

    def select(self, node):
        while node.children:
            if not all(child.visits for child in node.children):
                return self.expand(node)
            node = self.uct_select(node)
        return self.expand(node)

    def expand(self, node):
        moves = self.get_possible_moves(node.game)
        for move in moves:
            if move not in [child.move for child in node.children]:
                new_game = self.make_move(node.game, move)
                child = MCTSNode(new_game, parent=node, move=move)
                node.children.append(child)
                return child
        return node

    def simulate(self, game):
        temp_game = Game.from_dict(game.to_dict())
        simulation_depth = 0
        history = []  # To store states for Q-learning

        while temp_game.check_winner() == EMPTY and simulation_depth < 50:
            moves = self.get_possible_moves(temp_game)
            if not moves:
                break
            move = self.select_best_move(temp_game, moves)
            state = json.dumps(temp_game.to_dict())  # Convert the state to a JSON string
            history.append((state, move))
            self.make_move(temp_game, move)
            simulation_depth += 1

        reward = 1 if temp_game.check_winner() == self.player else 0

        # Update Q-values based on history
        for state, move in reversed(history):
            if state not in self.q_values:
                self.q_values[state] = {}
            move_tuple = tuple(move)  # Convert move list to tuple
            if move_tuple not in self.q_values[state]:
                self.q_values[state][move_tuple] = 0
            self.q_values[state][move_tuple] += self.learning_rate * (
                reward + self.discount_factor * max(self.q_values.get(state, {}).values(), default=0) - 
                self.q_values[state][move_tuple]
            )
        
        return reward

    def backpropagate(self, node, score):
        while node:
            node.visits += 1
            node.score += score
            node = node.parent

    def uct_select(self, node):
        log_n_visits = math.log(node.visits + 1)  # Add 1 to avoid log(0)
        return max(node.children, key=lambda c: c.score / (c.visits + 1) + 
                   self.exploration_weight * math.sqrt(log_n_visits / (c.visits + 1)))

    def get_possible_moves(self, game):
        moves = []
        current_pieces = game.p1_pieces if game.current_player == PLAYER1 else game.p2_pieces
        
        if current_pieces < NUM_PIECES:
            for r in range(BOARD_SIZE):
                for c in range(BOARD_SIZE):
                    if game.board[r][c] == EMPTY:
                        moves.append([r, c])
        else:
            for r0 in range(BOARD_SIZE):
                for c0 in range(BOARD_SIZE):
                    if game.board[r0][c0] == game.current_player:
                        for r1 in range(BOARD_SIZE):
                            for c1 in range(BOARD_SIZE):
                                if game.board[r1][c1] == EMPTY:
                                    moves.append([r0, c0, r1, c1])
        return moves

    def make_move(self, game, move):
        new_game = Game.from_dict(game.to_dict())
        if len(move) == 2:
            new_game.place_checker(move[0], move[1])
        else:
            new_game.move_checker(move[0], move[1], move[2], move[3])
        new_game.current_player *= -1
        return new_game

    def select_best_move(self, game, moves):
        best_move = None
        best_value = float('-inf')

        for move in moves:
            state = json.dumps(game.to_dict())  # Convert state to string for hashing
            move_tuple = tuple(move)  # Convert move list to tuple
            if state in self.q_values and move_tuple in self.q_values[state]:
                move_value = self.q_values[state][move_tuple]
            else:
                move_value = random.random()  # Exploration strategy (random value)
            
            if move_value > best_value:
                best_value = move_value
                best_move = move

        return best_move

    def get_random_valid_move(self, game):
        moves = self.get_possible_moves(game)
        return [int(x) for x in random.choice(moves)] if moves else None

    def train(self, num_games=1000):
        for _ in range(num_games):
            game = Game()
            while game.check_winner() == EMPTY:
                move = self.get_best_move(game)
                self.make_move(game, move)
            self.save_q_values()  # Save Q-values periodically during training
        print("Training complete.")

# Example Usage
if __name__ == "__main__":
    agent = MCTS2Agent()
    agent.train(num_games=500)
